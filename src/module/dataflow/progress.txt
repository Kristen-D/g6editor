
2019-01-02 开会了解建模平台。
2019-01-10 确定采用G6以及G6editor作为前端页面可视化拖拽组件开发。
2019-01-18 完成vue版的G6editor开发。
2019-01-25 熟悉了解Spring cloud data flow相关的知识点、以及流程，运行docker版SCDF用做测试。
2019-02-01 完成task任务模块的G6editor拖拽的流程图到DSL语句的转换。
2019-02-23 完成stream模块的G6editor拖拽的流程图到DSL语句的转换。
2019-03-01 完成建模平台的整体前端页面框架设计、交互页面的设计、以及Vue页面的编写
2019-03-10 完成task模块、stream模块的DSL语句的接口对接。
2019-03-19 完成整体页面的数据接口对接。
2019-03-28 完成流程测试以及流程完整。


ssa-source-b96117a7146f480181be4c75b33cc76d$67$:ssa-source --spark.executor.memory=512M --spark.driver.memory=512M --different-groupid=group --spark.driver.cores=1 --ssa_app_type=1 --spark.executor.cores=1 --bootstrap-servers=192.168.1.187:9092 --generate-topic=asap_superset --receive-topic=input --sync=false --spark.sql.shuffle.partitions=16 --timeout=6000 --streaming.batch_interval=6000 --streaming.stopSparkContext=true --consume-location=latest --ssa_app_id=67 --context-factory=spark.jobserver.context.StreamingContextFactory --streaming.stopGracefully=false --spark.serializer=org.apache.spark.serializer.JavaSerializer --source-type=kafka --spark.executor.instances=1 --flow_task_id=3f05f9aff4954b1a9b8e3c9d7446bc80 --kafka-batch-size=5000 && ssa-processor-84b0ddf4e0bd432a823f827533f73357$110$:ssa-processor --schema-string-null=eventLevel_s,reMark --ssa_app_type=1 --schema-string=data_index,logTime_dt,uuid,collectTime,equIP,sourceType,level,collectPro,filePath,id,msg_s,detectionComp_s,riskType_s,dstGroup_s,srcIp_CityName_s,deviceType_s,srcCity_Province_LngLat_s,direction_s,dstMAC_s,relationUUID_s,malwareType_s,fileType_s,ruleId_s,componentVersion_s,dstOrgId_s,externalIp_s,fileSize_s,aptRelated_s,riskLevel_s,threatType_s,dstCountryName_s,srcIp_dstIp_s,cccaDetectionSrc_s,srcOrgId_s,dstHostName_s,srcAsset_s,cccaDstIp_s,pver_s,detectionType_s,deviceName_s,protocol_s,dstLngLat_s,srcIp_Port_s,logSubType_s,channelName_s,eventName_s,vendor_s,cccaDetection_s,malwareFamily_s,srcIp_threatType_s,srcCityName_LngLat_s,dstIp_CityName_s,confidenceLevel_s,srcCityName_s,attackType_s,logType_s,peerIp_s,dstCityName_s,deviceMac_s,appType_s,dstProvinceName_s,dstPort_s,srcHostName_s,dstCityName_LngLat_s,srcIp_s,srcZone_s,srcCountryName_s,aggregatedCnt_s,cccaDirection_s,dstCity_Province_LngLat_s,srcProvinceName_s,deviceIp_s,srcPort_s,dstZone_s,pattackPhaseId_s,devTimeFormat_s,action_s,threatTypeDesc_s,protoGroup_s,dstIp_threatType_s,interestedIp_s,cccaType_s,filter_s,vlanId_s,dstIp_s,logVer_s,dstAsset_s,srcMac_s,pattackPhaseDesc_s,pattackPhase_s,botCmd_s,srcLngLat_s,deviceGUID_s,eventType_s,parentEventType_s,rawMsg,recordTime --source=HDF --spark.sql.shuffle.partitions=16 --batchsize=5000 --out-table=alarm_info --timeout=6000 --parquet=/user/asap/d1.parquet --duration=10 --bootstrap-server=192.168.1.187:9092 --password=1qazXSW@3edc --streaming.stopSparkContext=true --ssa_app_id=110 --context-factory=spark.jobserver.context.JavaStreamingContextFactory --streaming.stopGracefully=false --redis-port=6379 --spark.serializer=org.apache.spark.serializer.JavaSerializer --spark.executor.instances=1 --redis-password=1qazXSW@3edc --flow_task_id=4c2f75a9901b48159c6bde4f2e8f5c83 --spark.executor.memory=512M --partitions=20 --spark.driver.memory=512M --schema-integer=count_l --spark.driver.cores=1 --namedrdd=alarm_info --sink=Kafka --spark.executor.cores=1 --rule-file=/data/asap/asap-statistical-test/conf/asap.drl --schema-double=_version_ --alarm_info='SELECT nextId(null) AS uu_id, substring(\'logTime_dt\', 1, 13) as alarm_time，substring_index(concat_ws(collect_list(uuid))10) AS alarm_id_ws FROM log l GROUP BY substring(logTime_dt, 1, 13), data_index' --dimension-table=am_asset --g3='SELECT * FROM log l ' --sync=false --streaming.batch_interval=6000 --url=jdbc:mysql://192.168.1.26:3306/spark?characterEncoding=utf8&autoReconnect=true&rewriteBatchedStatements=true&jdbcCompliantTruncation=false --app-name=Asap-Statistical-Indicator-Test --alarm_info_ref=alarm_info_event --driver=com.mysql.jdbc.Driver --guid=62 --topic=asap_superset --redis-host=192.168.1.26 --SPARK_KAFKA_VERSION=0.10 --user=root && ssa-sink-034b1bdd9e73460184bd3016c8b6dc89$69$:ssa-sink --spark.executor.memory=512M --spark.driver.memory=512M --different-groupid=group --spark.driver.cores=1 --ssa_app_type=1 --spark.executor.cores=1 --bootstrap-servers=192.168.1.187:9092 --generate-topic=stat_out --receive-topic=alarm_info --sync=false --spark.sql.shuffle.partitions=16 --timeout=6000 --streaming.batch_interval=6000 --streaming.stopSparkContext=true --consume-location=latest --ssa_app_id=69 --context-factory=spark.jobserver.context.StreamingContextFactory --streaming.stopGracefully=false --spark.serializer=org.apache.spark.serializer.JavaSerializer --sink-type=kafka --spark.executor.instances=1 --flow_task_id=7cb982e2abd44ab2969bfac8f9041d13 --kafka-batch-size=5000
